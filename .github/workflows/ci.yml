name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install black ruff pytest
        pip install -r requirements.txt
    
    - name: Run black
      run: black --check src/ scripts/ tests/
    
    - name: Run ruff
      run: ruff check src/ scripts/ tests/
    
    - name: Run type checking
      run: |
        pip install mypy
        mypy src/ --ignore-missing-imports

  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run tests
      run: |
        python -m pytest tests/ -v --cov=src --cov-report=xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml

  inference-test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Generate synthetic data
      run: |
        python scripts/generate_synthetic_data.py --num_samples 10
    
    - name: Test model inference
      run: |
        python -c "
        import sys
        sys.path.append('src')
        from models.tacotron2_hifigan import Tacotron2HiFiGAN
        from omegaconf import OmegaConf
        import torch
        
        config = OmegaConf.load('configs/config.yaml')
        model = Tacotron2HiFiGAN(config.model)
        
        # Test inference
        text = torch.randint(0, 100, (1, 10))
        with torch.no_grad():
            outputs = model.inference(text)
        
        print('Inference test passed!')
        "
